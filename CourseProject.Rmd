---
title: "Predicting Quality of Dumbell Exercises Using Accelerometer Readings"
author: "Mark Rhine"
date: "Monday, May 18, 2015"
output: html_document
---

##Executive Summary
This report will create a predictive model that attempts to predict how well a participant performs a barbell excerise based solely on accelerometer readings. Specifically, I will show that a random forest model can accurately predict quality of a dumbell exercise. The quality outcome is a variable called 'classe' and it has 5 possible values. A value of 'A' equates to a perfect dumbell exercise and values 'B', 'C', 'D', and 'E' equate to different mistakes in the exercise. The random forest model will accurate predict a 'classe' value of 'A' through 'E'.


##Loading the Dataset

First, I loaded in the dataset that was saved on my hard drive. There were separate files for training and testing data.The dataset can be downloaded at http://groupware.les.inf.puc-rio.br/har.

```{r, cache=TRUE, warning=FALSE}
train <- read.csv("C:/Users/Mark Account/Documents/Software/Coursera/Machine Learning/pml-training.csv", na.strings=c("","NA"))
test <- read.csv("C:/Users/Mark Account/Documents/Software/Coursera/Machine Learning/pml-testing.csv")
library(caret)
library(randomForest)
```

##Cleaning the Dataset

The first thing I did was bring our dependent variable that we are trying to predict, 'classe', up to first column.

```{r, cache=TRUE}
train2 <- train[,c(160, 1:159)]
```

```{r, results='hide'}
summary(train2)
```

By looking at the summary data for the 'train2' dataset, it was clear that there were many variables that consisted mostly of missing values. Therefore, those variables are not valuable in predicting and so I disposed of them from the data frame. I set a threshold of 5,000 missing values per variable. If a variable column had over 5,000 missing values, it would be disposed of. Note, the amount of missing values per column was either 0 or 19,216, so the exact threshold was of little importance.

```{r, cache=TRUE}
#create copies of data frame
train3 <- train2
train4 <- train3

#create a vector of index numbers of columns with count of missing values over 5,000.
foo <- c()
for(x in 1:160){
    u <- is.na(train3[,x])
    b <- sum(u)
    if(b > 5000){
        #train4 <- train4[-x]
        foo <- c(foo, x)
        
    }
    
}
#remove those columns from training set
train5 <- train4[-foo]
```
There was a variable column that was a unique number identifier for each row. This number is of no importance to predicting the 'classe' outcome, so it too was removed.

```{r, cache=TRUE}
train6 <- train5[-2]
```
Now the 'training' dataset has only 60 variables, 59 of which are potentially important independent variables. But I still need to test my model. I couldn't test my model on the 'test' data set because that would lead to overfitting (and I only have two attempts to predict correctly). So I split the 'train' data frame into 'training' and 'testing' data frames. I will build the model on the 'training data frame and then test my model on the 'testing' data frame. 

```{r, cache=TRUE}
#random numbers for train & test
#create a random number vector between 1 and 19622, the number of rows in train set
#set seed
set.seed(1000)
randnumbers <- sample(1:19622, 19622, replace=F)

#Break up randnumbers, 70% (1 - 13735) and 30% (13735 - 19622.
t <- randnumbers[1:13735]
z <- randnumbers[13736:19622]

#split train data into training & testing set
training <- train6[t,]
testing <- train6[z,]
```


##Creating the Model
I have no experience using accelerometers or what type of relationship each measurement would have with the 'classe' dependent variable. So I choose to use a Random Forest model. It will create a tree model using the independent variables and is not interpretable to user, but that is okay. These measurements don't need to be interpretable. If I could accurately predict the outcome without gaining an understanding of the relationships between all of the 59 independent variables and the outcome, that would be sufficient for my purpose.

###Cross-Validation
In order to improve the model I used K-Fold cross validation. This divides the training set into 10 sections (folds), and creates a model on 9 folds, tests it on the remaing fold, and does this for each fold. Then it averages the 10 models created into 1, giving me a better model. So in the Caret library, I set trainControl to cross validation and 10 folds.

```{r, cache=TRUE}
train_control <- trainControl(method="cv", number=10)
```
I then built my first model using a random forest technique. 

```{r, cache=TRUE}
set.seed(1000)
model1 <- train(classe ~ ., data=training, trControl = train_control, method="rf")
```

##Testing the Model
My first test was to check the accuracy of the model on the 'training' data frame. There were no errors when using the model to predict 'classe' using the 'training' data. The in-sample error rate is 0%. This is great but could also mean that the model has overfit the 'training' data.

```{r, cache=TRUE}
#In-Sample Error Rate
predTrain <- predict(model1, newdata=training)
table(training$classe, predTrain)
```

Next, I tested the model on the 'testing' data. Note, this 'testing' data is different than the 'test' data where I do not know the outcome variable. The 'testing' data frame was subsetted from the 'train' set, so I know the outcome and can use it to estimate the out-of-sample error rate.
```{r, cache=TRUE}
#Estimated Out of Sample Error Rate
predRF <- predict(model1, newdata=testing)
table(testing$classe, predRF)
```

```{r}
#2 wrong out of 5887 data values.
2/5887
```

At .03397% estimated out-of-sample error rate, this model seems extremely good, and is ready to be used to predict the true test data 'classe' outcome. There was no need to search for a better model!
